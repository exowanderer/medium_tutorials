# -*- coding: utf-8 -*-
"""PytorchBNNMedium.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R6kjCUvLWUjEDep5tkrD6smzB2btmZ29

Source: https://medium.com/towards-data-science/from-theory-to-practice-with-bayesian-neural-network-using-python-9262b611b825
"""

# !pip install torchbnn

"""The first thing to do is to import some libraries:"""

import numpy as np
from sklearn import datasets
import torch
import torch.nn as nn
import torch.optim as optim
import torchbnn as bnn
import matplotlib.pyplot as plt

"""After that, we will make our very simple bidimensional dataset:"""

def clean_target(x):
    return x.pow(5) -10* x.pow(1)
def target(x, nsig=2):
    return clean_target(x) + nsig*torch.normal(0, 1, x.size())

x = torch.linspace(-2, 2, 500)
# y = x.pow(5) -10* x.pow(1) + 20*torch.normal(x.size())
y = target(x, nsig=5)
x = torch.unsqueeze(x, dim=1)
y = torch.unsqueeze(y, dim=1)

plt.scatter(x.data.numpy(), y.data.numpy())
plt.show()

"""So, given our 1D input x (ranging from -2 to 2), we want to find our y.

Clean_target is our ground truth generator, and target is our noisy data geneator.

Now we will define our Bayesian feed-forward neural network:
"""

model = nn.Sequential(
    bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=1, out_features=1000),
    nn.ReLU(),
    bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=1000, out_features=1),
)

"""As we can see, it is a two-layer feed-forward neural network with Bayesian layers. This will allow us to have a probabilistic output.

Now we will define our MSE loss and the remaining Kullback-Leibler divergence:
"""

mse_loss = nn.MSELoss()
kl_loss = bnn.BKLLoss(reduction='mean', last_layer_only=False)
kl_weight = 0.01

optimizer = optim.Adam(model.parameters(), lr=0.01)

"""Both the losses will be used in our optimization step:"""

n_iter_bnn = 10000
for step in range(n_iter_bnn):
    pre = model(x)
    mse = mse_loss(pre, y)
    kl = kl_loss(model)
    cost = mse + kl_weight*kl
    
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()
    
print('- MSE : %2.2f, KL : %2.2f' % (mse.item(), kl.item()))

"""2000 epochs have been used.

Letâ€™s define our test set:
"""

x_test = torch.linspace(-2, 2, 300)
y_test = target(x_test)

x_test = torch.unsqueeze(x_test, dim=1)
y_test = torch.unsqueeze(y_test, dim=1)

"""Now, the result that comes out of the model class is probabilistic. This means that if we run our model 10,000 times, we will get 10,000 slightly different values. For each data point from -2 to 2, we will get the mean and standard deviation,"""

models_result = np.array([model(x_test).data.numpy() for k in range(10000)])
models_result = models_result[:,:,0]    
models_result = models_result.T
mean_values = np.array([models_result[i].mean() for i in range(len(models_result))])
std_values = np.array([models_result[i].std() for i in range(len(models_result))])

"""and we will plot our confidence intervals."""

plt.figure(figsize=(10,8))
plt.plot(x_test.data.numpy(),mean_values,color='navy',lw=3,label='Predicted Mean Model')
plt.fill_between(x_test.data.numpy().T[0],mean_values-3.0*std_values,mean_values+3.0*std_values,alpha=0.2,color='navy',label='99.7% confidence interval')
#plt.plot(x_test.data.numpy(),mean_values,color='darkorange')
plt.plot(x_test.data.numpy(),y_test.data.numpy(),'.',color='darkorange',markersize=4,label='Test set')
plt.plot(x_test.data.numpy(),clean_target(x_test).data.numpy(),color='green',markersize=4,label='Target function')
plt.legend()
plt.xlabel('x')
plt.ylabel('y')